{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991aa33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "import random\n",
    "def train_model(Q_copy, env_inv, num_episodes = 10000,  epsilon_final= 0.1, epsilon = 0.8, epsilon_decay = 0.999,  learning_rate = 0.9,\n",
    "            gamma = 0.90 ,   to_print = True):\n",
    "    steps_total_inv = [] # store number of steps taken in each episode\n",
    "    rewards_total_inv = [] #store reward obtained for each episode\n",
    "    epsilon_total_inv = [] #store epsilon obtained at the end of each episode\n",
    "    terminal_state_inv = []\n",
    "    if len(Q_copy) == 0:\n",
    "        Q_copy = pd.DataFrame(np.random.rand(n_states,n_actions)/1000)\n",
    "        Q_copy.loc[15] = np.zeros(n_actions,)\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # resets the environment\n",
    "        state = env_inv.reset()\n",
    "        step = 0\n",
    "        #reward = 0\n",
    "\n",
    "      ## as epsilon decays with more timesteps, the prob. of selecting a random val < e decays --> more likely to exploit. \n",
    "        if epsilon > epsilon_final:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "        while True:\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            random_for_epsilon = np.random.rand()\n",
    "            if random_for_epsilon <= epsilon:\n",
    "              action = env_inv.action_space.sample()\n",
    "            else: \n",
    "              action = np.argmax(Q_copy.loc[state])\n",
    "\n",
    "\n",
    "            ## env gives reward and next state and whether we've reached terminal state upon taking action at current state.. \n",
    "            new_state, reward , done, info = env_inv.step(action)\n",
    "\n",
    "            ##if you want reward penalized at for each timestep\n",
    "            #reward = rewarder(new_state, reward)\n",
    "\n",
    "            # filling the Q Table - \n",
    "\n",
    "            Q_copy.loc[state, action] = (1- learning_rate)*Q_copy.at[state, action] + learning_rate*(reward + gamma * np.max(Q_copy.loc[new_state]))\n",
    "\n",
    "            # Setting new state for next action\n",
    "            state = new_state\n",
    "            tile = og_4x4_inv[rowsandcols(state)[0]][rowsandcols(state)[1]]\n",
    "            #env.render()\n",
    "\n",
    "            if done:\n",
    "              #print(Q)\n",
    "\n",
    "              terminal_state_inv.append(tile)\n",
    "              steps_total_inv.append(step)\n",
    "              rewards_total_inv.append(reward)\n",
    "              #print(reward)\n",
    "              epsilon_total_inv.append(epsilon)\n",
    "              if to_print:\n",
    "                  if i_episode % 10 == 0:\n",
    "                    print('Episode: {} Reward: {} Steps Taken: {} Terminal State: {}, Epsilon: {}'.format(i_episode,reward, step, tile, epsilon))\n",
    "              if i_episode == num_episodes:\n",
    "                print('Mean Award %g'%np.mean(rewards_total_inv))\n",
    "            \n",
    "              break\n",
    "    return terminal_state_inv, steps_total_inv, rewards_total_inv, epsilon_total_inv, Q_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149895c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_actions(Q, n_states, n_rows):\n",
    "\n",
    "  Q.at[n_states -1, :] = np.zeros(n_rows,)\n",
    "  for i in range( 0, n_states, n_rows): \n",
    "    Q.at[i,0] = np.NaN\n",
    "  for i in range( n_rows -1 , n_states, n_rows): \n",
    "    Q.at[i,2] = np.NaN\n",
    "  for i in range(0, n_rows):\n",
    "    Q.at[i,3] = np.NaN\n",
    "  for i in range(n_states - n_rows , n_states):\n",
    "    Q.at[i,1 ]= np.NaN\n",
    "  \n",
    "  return Q\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(Q, state, epsilon):\n",
    "  random_for_epsilon = np.random.rand()\n",
    "  if random_for_epsilon <= epsilon:\n",
    "    s = Q.loc[state].notna()\n",
    "    vals = s[s].index.values\n",
    "    action = random.choice(vals)\n",
    "  else: \n",
    "    Q.loc[state] += np.random.rand(n_actions,)/100\n",
    "    action = np.argmax(Q.loc[state])\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowsandcols(state):\n",
    "  ''' input: state returned by env\n",
    "      output: location of state as (row,col) tuple'''\n",
    "  return int(np.where(state_matrix ==state)[0]), int(np.where(state_matrix ==state)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928821b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewarder(new_state, reward):\n",
    "  if og_4x4[rowsandcols(new_state)[0]][rowsandcols(new_state)[1]]== 'H':\n",
    "    reward -= 20\n",
    "  elif og_4x4[rowsandcols(new_state)[0]][rowsandcols(new_state)[1]]== 'F':\n",
    "    reward -= 1 \n",
    "  elif og_4x4[rowsandcols(new_state)[0]][rowsandcols(new_state)[1]]== 'S':\n",
    "    reward -= 1\n",
    "  else: #goal\n",
    "    reward += 100\n",
    "\n",
    "  return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_transfer(env,env2, num_episodes = 10000,  epsilon_final= 0.1, epsilon = 0.8, epsilon_decay = 0.999,  learning_rate = 0.9,\n",
    "                   gamma = 0.90 ,num_episodes2 =  10000 , epsilon2 = 0.8,r_seed = 90,to_print = False):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    np.random.seed(r_seed)\n",
    "    Q = pd.DataFrame(np.random.rand(n_states,n_actions)/1000)\n",
    "    Q.loc[15] = np.zeros(n_actions,)\n",
    "    terminal_state, steps_total, rewards_total, epsilon_total_inv, Q = train_model(Q , env, num_episodes,  epsilon_final, epsilon , epsilon_decay ,  learning_rate,  gamma,to_print = to_print  )        \n",
    "    Q_copy = Q.copy()\n",
    "    terminal_state_inv, steps_total_inv, rewards_total_inv, epsilon_total_inv, Q_new = train_model(Q_copy, env2, num_episodes2,  epsilon_final, epsilon2 , epsilon_decay ,  learning_rate,  gamma  ,  to_print = to_print)   \n",
    "    return Q, Q_new, rewards_total,steps_total,  rewards_total_inv,steps_total_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movmean(sig, wind = 10):\n",
    "    av_sig = [np.mean(sig[i-int(wind/2) : i+int(wind/2)]) for i in np.arange(len(sig)-wind)+int(wind/2)+1]\n",
    "    return av_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_env_transform(board, transform_type = 'transpose',params = {'miss_num':2}, unique_board = np.array(['F','H'])):\n",
    "    if transform_type == 'transpose':\n",
    "         tra_board = [''.join([word[i] for word in og_4x4])  for i in range(len(board[0]))]\n",
    "    elif transform_type == 'miss':\n",
    "        #if isinstance(board,list): board = np.vstack(board)\n",
    "        board_new = np.vstack([[letter for letter in row] for row in  board]  )\n",
    "        pairs = list(itertools.product(np.arange(board_new.shape[0]), np.arange(board_new.shape[1])))\n",
    "        pairs = pairs[1:-1]\n",
    "        sample_random = random.sample(pairs,params['miss_num'])\n",
    "        if len(unique_board) == 0: unique_board = np.unique(board)\n",
    "        tra_board = board_new.copy()\n",
    "        #print(sample_random)\n",
    "        #display(board_new)\n",
    "        for pair_choose in sample_random:\n",
    "            #print(pair_choose)\n",
    "            #print(pair_choose)\n",
    "            \n",
    "            cur_value = board_new[pair_choose[0],pair_choose[1]][0]\n",
    "\n",
    "            new_val = random.choice(unique_board[unique_board != cur_value])\n",
    "\n",
    "            tra_board[pair_choose[0],pair_choose[1]] = new_val[0]\n",
    "    else:\n",
    "        raise NameError('Unknown transform type')\n",
    "    return tra_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b910e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_array_to_list(board_array):\n",
    "    return [''.join(row) for row in board_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs = list(itertools.combinations(np.arange(num_subdyns),k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e736333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_nums(board,dict_trans = {'F' : 1, 'G' : 2, 'H': 3, 'S' : 4}):\n",
    "    if isinstance(board,list):\n",
    "        board = np.vstack([[letter for letter in row] for row in  board]  )\n",
    "        #board = np.vstack(board)\n",
    "    board_num = np.nan*np.zeros(np.array(board).shape)\n",
    "    #print(board_num)\n",
    "    for row_num, word in enumerate(board):\n",
    "        for col_num, letter in enumerate(word):\n",
    "            new_val = dict_trans[letter]\n",
    "            board_num[row_num,col_num] = new_val\n",
    "    return board_num\n",
    "                                    \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
